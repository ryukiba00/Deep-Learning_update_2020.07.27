{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6-1_PreProcessing_update_2020.05.02.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP/2ud621+UFDv7XtEOZ5+j"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DIoQARcQVYCz","colab_type":"text"},"source":["#**6-1 文件的預處理**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QRHcTmimXr0u","colab_type":"text"},"source":["*    **向量化**，原始文件無法輸入至模型，需轉成數值張量"]},{"cell_type":"markdown","metadata":{"id":"lJSEcbejVqex","colab_type":"text"},"source":["*   舉例過程: Text(文件) -> Tokens -> Vector encoding of the tokens\n","\n","\n","\n","    ---  Token: 小單元，通常是文件字串裡分隔開的單詞\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FhU5LgNcWAD_","colab_type":"text"},"source":["*   n元語法: 請參考6-4"]},{"cell_type":"markdown","metadata":{"id":"l5aBfHO4XPNo","colab_type":"text"},"source":["\n","\n","*   **轉換成向量的方法**\n","1.   One Hot Encoding\n","2.   Token Embedding Token 嵌入法\n","\n"]},{"cell_type":"markdown","metadata":{"id":"A4070eJUX7AG","colab_type":"text"},"source":["\n","\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"onge881PX7KJ","colab_type":"text"},"source":["# **6-1-1 單字和字元的One Hot Encoding**"]},{"cell_type":"markdown","metadata":{"id":"zMjxa0qDZcYO","colab_type":"text"},"source":["    ---   本章節使用的案例為第3章提過的IDMB資料集\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BTuAx1MwdHIB","colab_type":"text"},"source":["程式 6.1 單字的One-Hot Encoding(簡易版) 未處理標點符號"]},{"cell_type":"code","metadata":{"id":"IpDZPgmyZtqi","colab_type":"code","outputId":"77d1c7b8-f32f-45ae-98e8-64a359279a7b","executionInfo":{"status":"ok","timestamp":1588303557044,"user_tz":-480,"elapsed":1119,"user":{"displayName":"王文友105","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx3h3fbIXhmCICsGKVqtwWmrgA4DxMJylZtLz9=s64","userId":"15968277035213818466"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["import numpy as np\n","\n","samples = ['The cat sat on the mat.','The dog ate my homework.']\n","\n","# 空dict，儲存tokens和tokens的key\n","token_index = {}\n","\n","# 建立字典\n","for sample in samples:\n","  for word in sample.split():\n","    if word not in token_index:\n","      token_index[word] = len(token_index) + 1\n","\n","# 將token轉成向量\n","max_length = 10\n","# 建立Numpy Array，使用numpy.zeros()先將所有設為0\n","results = np.zeros(shape=(len(samples), max_length, max(token_index.values())+1))\n","# shape = (2, 10, 11)，2個樣本；只看前10個文字，所以有10個token；索引位置至11，因為0不用看\n","#print(results.shape)\n","\n","for i, sample in enumerate(samples):\n","  for j, word in list(enumerate(sample.split()))[:max_length]:\n","    index = token_index.get(word)\n","    results[i, j, index] = 1\n","#print(token_index)\n","print(results)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n","\n"," [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bjvmmcy8RU3z","colab_type":"text"},"source":["程式 6.2 字元的One-Hot Encoding(簡易版) 未處理標點符號"]},{"cell_type":"code","metadata":{"id":"2bxrH7psfPTM","colab_type":"code","outputId":"cea67f33-66c2-4572-febd-70e2b5b8ebef","executionInfo":{"status":"ok","timestamp":1588304215996,"user_tz":-480,"elapsed":1051,"user":{"displayName":"王文友105","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx3h3fbIXhmCICsGKVqtwWmrgA4DxMJylZtLz9=s64","userId":"15968277035213818466"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["import string\n","\n","samples = ['The cat sat on the mat.','The dog ate my homework.'] \n","characters = string.printable         # 所有可印出的ASCII字元的字串\n","#print(len(characters))                # 共有100種token\n","\n","token_index = dict(zip(characters, range(1, len(characters)+1)))\n","#print(token_index)\n","\n","max_length = 50\n","results = np.zeros((len(samples),max_length, max(token_index.values())+1))\n","\n","#print(results.shape)                     # (2, 50, 101),2個樣本,只看樣本前50字,100種token，0不使用\n","for i, sample in enumerate(samples):\n","  for j, character in enumerate(sample):\n","    index = token_index.get(character)\n","    results[i, j , index] = 1\n","print(results)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[[0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]]\n","\n"," [[0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SENV6VLFUVe0","colab_type":"text"},"source":["程式 6-3 使用keras作文字的encoding"]},{"cell_type":"code","metadata":{"id":"eYWE7CZNSfhp","colab_type":"code","outputId":"39d15fa9-db6d-4167-8d7f-3e1b28d92bfc","executionInfo":{"status":"ok","timestamp":1589200462461,"user_tz":-480,"elapsed":2172,"user":{"displayName":"王文友105","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx3h3fbIXhmCICsGKVqtwWmrgA4DxMJylZtLz9=s64","userId":"15968277035213818466"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from keras.preprocessing.text import Tokenizer\n","\n","samples = ['The cat sat on the mat.','The dog ate my homework.']\n","\n","tokenizer = Tokenizer(num_words=1000)           # 建立分詞器，處理前1000個單字\n","tokenizer.fit_on_texts(samples)                 # 建立dict,key_value由出現頻度以及順率決定,不使用0；# fit_on_text,設定成用來訓練的txt\n","sequences = tokenizer.texts_to_sequences(samples)# 將smaples裡字串的每個單字用dict轉換成整數鑑值，return a list；text_to_sequence,將txt轉換為序列\n","#print(sequences)\n","\n","one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary') # texts_to_matrix,轉換為向量\n","\n","#print(one_hot_results.shape)                     # (2, 1000),2個樣本,sample的文字對應到token位置\n","#print(one_hot_results[0][:15])\n","#print(one_hot_results[1][:15])\n","\n","word_index = tokenizer.word_index                 # 取得單字與索引間的對應關聯      \n","print(len(word_index))                            # 找到9個唯一的tokens"],"execution_count":0,"outputs":[{"output_type":"stream","text":["9\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"k391SPc3YE91","colab_type":"text"},"source":["程式 6-4 使用雜湊技巧One-hot hashing trick的單字(簡易版),用來處理token數量太大"]},{"cell_type":"code","metadata":{"id":"h0cSgUamWOUL","colab_type":"code","outputId":"b092f8c8-0271-4d0a-ab13-6d14a0c6a45e","executionInfo":{"status":"ok","timestamp":1588306095841,"user_tz":-480,"elapsed":1206,"user":{"displayName":"王文友105","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx3h3fbIXhmCICsGKVqtwWmrgA4DxMJylZtLz9=s64","userId":"15968277035213818466"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["import numpy as np\n","\n","dimensionality = 1000                   # 將token儲存維1000的向量\n","max_length = 10\n","\n","\n","results = np.zeros((len(samples), max_length, dimensionality))\n","for i, sample in enumerate(samples):\n","  for j, word in list(enumerate(sample.split()))[:max_length]:\n","    index = abs(hash(word)) % dimensionality\n","    results[i, j, index] = 1\n","#print(results.shape)\n","print(results)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[[0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]]\n","\n"," [[0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nCPHjIIB9SMU","colab_type":"text"},"source":["程式 6-5 建立一個嵌入層"]},{"cell_type":"code","metadata":{"id":"x5QHgolXa1ir","colab_type":"code","colab":{}},"source":["from keras.layers import Embedding\n","embedding_layer = Embedding(1000, 64) # 1000為可能的token數，64為要輸入的向量維數"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CcrpLQxX-lYp","colab_type":"text"},"source":["程式 6-6 載入IMDB Data Sets，整理成適合供Embedding層使用的資料"]},{"cell_type":"code","metadata":{"id":"pKh1owxP9hhx","colab_type":"code","outputId":"18713ec5-7098-4b64-cd32-0de9c68e18d6","executionInfo":{"status":"ok","timestamp":1588416616039,"user_tz":-480,"elapsed":5727,"user":{"displayName":"王文友105","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx3h3fbIXhmCICsGKVqtwWmrgA4DxMJylZtLz9=s64","userId":"15968277035213818466"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["from keras.datasets import imdb\n","from keras import preprocessing\n","\n","max_features = 1000 # 設定作為特徵的最常用文字數量\n","max_len = 20 # 只用每篇文章最前面的20個字\n","\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)\n","\n","#print(x_train.shape) # (25000,)為25000筆評論(樣本)\n","# 將list轉換為2d整數張量, shape為(sample數量, 最大長度)\n","x_train = preprocessing.sequence.pad_sequences(x_train, maxlen = max_len)\n","#print(x_train.shape)  # (25000, 20)只看前20個字\n","#print(x_train[0])\n","\n","x_test = preprocessing.sequence.pad_sequences(x_test, maxlen = max_len)\n","#print(x_test)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[286 170   8 ...  14   6 717]\n"," [ 10  10 472 ... 125   4   2]\n"," [ 34   2  45 ...   9  57 975]\n"," ...\n"," [226  20 272 ...  21 846   2]\n"," [ 55 117 212 ...   2   7 470]\n"," [ 19  14  20 ...  34   2   2]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B1DxTA5jAym4","colab_type":"text"},"source":["程式 6-7 把imdb資料提供給Embedding Layer和分類器"]},{"cell_type":"code","metadata":{"id":"YJ5-JL2D_UX4","colab_type":"code","outputId":"5c40398a-d0b1-426d-c684-78e7b3d2eb10","executionInfo":{"status":"ok","timestamp":1588417114386,"user_tz":-480,"elapsed":13689,"user":{"displayName":"王文友105","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx3h3fbIXhmCICsGKVqtwWmrgA4DxMJylZtLz9=s64","userId":"15968277035213818466"}},"colab":{"base_uri":"https://localhost:8080/","height":666}},"source":["from keras.models import Sequential\n","from keras.layers import Flatten, Dense, Embedding\n","\n","model = Sequential()\n","model.add(Embedding(10000, 8, input_length = max_len)) # 8為輸入維度\n","model.add(Flatten()) # 展平為2d向量\n","\n","model.add(Dense(1, activation = 'sigmoid'))\n","model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n","model.summary()\n","\n","history = model.fit(x_train, y_train, epochs = 10, batch_size = 32, validation_split = 0.2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_4 (Embedding)      (None, 20, 8)             80000     \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 160)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 161       \n","=================================================================\n","Total params: 80,161\n","Trainable params: 80,161\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 20000 samples, validate on 5000 samples\n","Epoch 1/10\n","20000/20000 [==============================] - 2s 79us/step - loss: 0.6798 - acc: 0.5842 - val_loss: 0.6441 - val_acc: 0.6794\n","Epoch 2/10\n","20000/20000 [==============================] - 1s 62us/step - loss: 0.5789 - acc: 0.7236 - val_loss: 0.5539 - val_acc: 0.7068\n","Epoch 3/10\n","20000/20000 [==============================] - 1s 62us/step - loss: 0.5114 - acc: 0.7498 - val_loss: 0.5306 - val_acc: 0.7240\n","Epoch 4/10\n","20000/20000 [==============================] - 1s 61us/step - loss: 0.4872 - acc: 0.7641 - val_loss: 0.5258 - val_acc: 0.7316\n","Epoch 5/10\n","20000/20000 [==============================] - 1s 64us/step - loss: 0.4752 - acc: 0.7704 - val_loss: 0.5253 - val_acc: 0.7358\n","Epoch 6/10\n","20000/20000 [==============================] - 1s 65us/step - loss: 0.4664 - acc: 0.7759 - val_loss: 0.5284 - val_acc: 0.7326\n","Epoch 7/10\n","20000/20000 [==============================] - 1s 61us/step - loss: 0.4580 - acc: 0.7807 - val_loss: 0.5300 - val_acc: 0.7312\n","Epoch 8/10\n","20000/20000 [==============================] - 1s 62us/step - loss: 0.4502 - acc: 0.7886 - val_loss: 0.5339 - val_acc: 0.7300\n","Epoch 9/10\n","20000/20000 [==============================] - 1s 63us/step - loss: 0.4427 - acc: 0.7915 - val_loss: 0.5384 - val_acc: 0.7276\n","Epoch 10/10\n","20000/20000 [==============================] - 1s 61us/step - loss: 0.4355 - acc: 0.7959 - val_loss: 0.5420 - val_acc: 0.7276\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xoTxal7oHJly","colab_type":"text"},"source":["!Error 程式 6-8, 處理原始IMDB資料的標籤"]},{"cell_type":"code","metadata":{"id":"HDkRfiJsCPnN","colab_type":"code","outputId":"ad72a194-1aae-4863-aa1b-37669971c276","executionInfo":{"status":"error","timestamp":1588419148615,"user_tz":-480,"elapsed":723,"user":{"displayName":"王文友105","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx3h3fbIXhmCICsGKVqtwWmrgA4DxMJylZtLz9=s64","userId":"15968277035213818466"}},"colab":{"base_uri":"https://localhost:8080/","height":253}},"source":["import os\n","from google.colab import files\n","\n","imdb_dir = 'D:/DataSets_for_Python/aclImdb'\n","train_dir = os.path.join(imdb_dir, 'train')\n","\n","labels = []\n","texts = []\n","\n","for label_type in ['neg', 'pos']:\n","    dir_name = os.path.join(train_dir, label_type)\n","    for fname in os.listdir(dir_name):\n","        if fname[-4:] == '.txt':\n","            f = open(os.path.join(dir_name, fname), encoding = 'utf8')\n","            texts.append(f.read())\n","            f.close()\n","            if label_type == 'neg':\n","                labels.append(0)\n","            else:\n","                labels.append(1)\n","print(len(texts))\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-888f1c3e2612>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdir_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/DataSets_for_Python/aclImdb/train/neg'"]}]},{"cell_type":"markdown","metadata":{"id":"BKSo2y8LJDc1","colab_type":"text"},"source":["!Error 程式 6-9 對原始IMDB資料的文字資料進行向量化"]},{"cell_type":"code","metadata":{"id":"63CeOUMOH6CS","colab_type":"code","outputId":"ae63e54b-0d38-4873-c74f-bd364d4ef459","executionInfo":{"status":"ok","timestamp":1588419686318,"user_tz":-480,"elapsed":866,"user":{"displayName":"王文友105","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx3h3fbIXhmCICsGKVqtwWmrgA4DxMJylZtLz9=s64","userId":"15968277035213818466"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","maxlen = 100  # 100 個文字後切斷評論 (只看評論的前 100 個字)\n","training_samples = 200  # 以 200 個樣本進行訓練\n","validation_samples = 10000 # 以 10, 000 個樣本進行驗證\n","max_words = 10000  # 僅考慮資料集中的前 10, 000 個單詞\n","\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequences(texts) # 將文字轉成整數 list 的序列資料\n","\n","word_index = tokenizer.word_index\n","print(word_index[: 10])\n","print('共使用了 %s 個 token 字詞.' % len(word_index))\n","\n","data = pad_sequences(sequences, maxlen=maxlen) # 只取每個評論的前 100 個字 (多切少補) 作為資料張量\n","labels = np.asarray(labels)  # 將標籤 list 轉為 Numpy array (標籤張量)\n","\n","print('資料張量 shape:', data.shape) # (25000, 100)\n","print('標籤張量 shape:', labels.shape) # (25000,)\n","\n","indices = np.arange(data.shape[0])  # 將資料拆分為訓練集和驗證集, 但首先要將資料打散, 因為所處理的資料是有順序性的樣本資料 (負評在前, 然後才是正評)\n","np.random.shuffle(indices)\n","data = data[indices]\n","labels = labels[indices]\n","\n","x_train = data[:training_samples]\n","y_train = labels[:training_samples]\n","x_val = data[training_samples: training_samples + validation_samples]\n","y_val = labels[training_samples: training_samples + validation_samples]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Q-uXH0nvMhkF","colab_type":"text"},"source":["!Error 程式 6-10 解析Glove文字嵌入向量檔案"]},{"cell_type":"code","metadata":{"id":"CtGJ0ePmMJ8V","colab_type":"code","outputId":"7718248a-e105-4f32-e3f3-af84b6366122","executionInfo":{"status":"error","timestamp":1588420575863,"user_tz":-480,"elapsed":920,"user":{"displayName":"王文友105","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx3h3fbIXhmCICsGKVqtwWmrgA4DxMJylZtLz9=s64","userId":"15968277035213818466"}},"colab":{"base_uri":"https://localhost:8080/","height":253}},"source":["glove_dir = 'D:\\DataSets_for_Python\\glove.6B'\n","\n","embedding_index = {}\n","\n","f = open(os.path.join(glove_dir, 'glove.68.100d.txt'))\n","\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.asarray(values[1:], dtype='float32')\n","  embedding_index[word] = coefs\n","f.close()\n","\n","print(len(embedding_index))"],"execution_count":0,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-66772289e509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0membedding_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'glove.68.100d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\DataSets_for_Python\\\\glove.6B/glove.68.100d.txt'"]}]},{"cell_type":"markdown","metadata":{"id":"Vsu5Psi_O7hb","colab_type":"text"},"source":["!Error 程式 6-11 準備GloVe文字嵌入向量矩陣"]},{"cell_type":"code","metadata":{"id":"RAKUnl5QO5A4","colab_type":"code","outputId":"c7bd8bc1-d985-47fa-e7da-9958b7946ee7","executionInfo":{"status":"ok","timestamp":1588421016080,"user_tz":-480,"elapsed":805,"user":{"displayName":"王文友105","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx3h3fbIXhmCICsGKVqtwWmrgA4DxMJylZtLz9=s64","userId":"15968277035213818466"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["embedding_dim = 100\n","\n","embedding_matrix = np.zeros((max_words, embedding_dim))\n","print(embedding_matrix.shape)\n","\n","for word, i in word_index.items():\n","  if i < max_words:\n","    embedding_vector = embedding_index.get(word)\n","    if embedding_vector is not Noe:\n","      embedding_matrix[i] = embedding_vector"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(10000, 100)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0kEZaDCDRYIN","colab_type":"text"},"source":["!Error 程式 6-12 定義模型"]},{"cell_type":"code","metadata":{"id":"CW4SN0bLRTZ7","colab_type":"code","outputId":"0eb977f0-75b4-45d2-adda-9c7683f00b90","executionInfo":{"status":"error","timestamp":1588421210157,"user_tz":-480,"elapsed":557,"user":{"displayName":"王文友105","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx3h3fbIXhmCICsGKVqtwWmrgA4DxMJylZtLz9=s64","userId":"15968277035213818466"}},"colab":{"base_uri":"https://localhost:8080/","height":369}},"source":["from keras.models import Sequentail\n","from keras.layers import Embedding, Flatten, Dense\n","\n","model = Sequential()\n","\n","model.add(Embedding(max_words, embedding_dim, input_length = max_len))\n","model.add(Flatten())\n","model.add(Dense(32, activation = 'relu'))\n","model.add(Dense(1, activation = 'sigmoid'))\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-a994f1b108ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequentail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'Sequentail'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"HaCHSXeESGyz","colab_type":"text"},"source":["程式 6-13 訓練和驗證"]},{"cell_type":"code","metadata":{"id":"7uq_HcmfSC12","colab_type":"code","outputId":"4f0fbad5-e643-4386-ea71-780dfbeb590d","executionInfo":{"status":"error","timestamp":1588421373996,"user_tz":-480,"elapsed":706,"user":{"displayName":"王文友105","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx3h3fbIXhmCICsGKVqtwWmrgA4DxMJylZtLz9=s64","userId":"15968277035213818466"}},"colab":{"base_uri":"https://localhost:8080/","height":219}},"source":["model.compile(optimizer = 'rmsprop',loss='binary_crossentropy',metrics = ['acc'])\n","history = model.fit(x_train, y_train, epochs = 10, batch_size = 32,validation_data = [x_val, y_val])\n","model.save_weights('pre_trained_glove_model.h5')"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-de8b9f72bfd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pre_trained_glove_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'x_val' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"xSdih0xLSsG8","colab_type":"text"},"source":["程式 6-14 繪製結果"]},{"cell_type":"code","metadata":{"id":"RsMReLg4Sqyi","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QHQaroUJT5-J","colab_type":"text"},"source":["程式 6.15 訓練相同模型而不使用預先訓練的文字嵌入向量"]},{"cell_type":"code","metadata":{"id":"LG7RlJ6-T98R","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Embedding, Flatten, Dense\n","\n","model = Sequential()\n","model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n","model.add(Flatten())\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()\n","\n","model.compile(optimizer='rmsprop',\n","              loss='binary_crossentropy',\n","              metrics=['acc'])\n","\n","history = model.fit(x_train, y_train,\n","                    epochs=10,\n","                    batch_size=32,\n","                    validation_data=(x_val, y_val))\n","\n","\n","############### 繪製\n","import matplotlib.pyplot as plt\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q8iO4k3fUBrj","colab_type":"text"},"source":["程式 6.16 對測試資料進行分詞"]},{"cell_type":"code","metadata":{"id":"QFZN4khPUIEJ","colab_type":"code","colab":{}},"source":["test_dir = os.path.join(imdb_dir, 'test')\n","\n","labels = []\n","texts = []\n","\n","for label_type in ['neg', 'pos']:\n","\tdir_name = os.path.join(test_dir, label_type)\n","\tfor fname in sorted(os.listdir(dir_name)):\n","\t\tif fname[-4:] == '.txt':\n","\t\t\tf = open(os.path.join(dir_name, fname), encoding='UTF-8')\n","\t\t\ttexts.append(f.read())\n","\t\t\tf.close()\n","\t\t\tif label_type == 'neg':\n","\t\t\t\tlabels.append(0)\n","\t\t\telse:\n","\t\t\t\tlabels.append(1)\n","\n","sequences = tokenizer.texts_to_sequences(texts)\n","x_test = pad_sequences(sequences, maxlen=maxlen)\n","y_test = np.asarray(labels)\n","\n","model.load_weights('pre_trained_glove_model.h5')\n","model.evaluate(x_test, y_test)"],"execution_count":0,"outputs":[]}]}